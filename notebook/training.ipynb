{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adf989e-7035-4576-9911-118750d1d4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 â€” Install light dependencies (run once)\n",
    "# (If you already have nltk or emoji, pip will skip/reuse them)\n",
    "!pip install nltk emoji sentence-transformers xgboost scikit-learn imbalanced-learn --quiet\n",
    "\n",
    "# Then in a new cell:\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568b883b-97d6-4359-938e-df2e607eac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Z shape: (299, 2)\n",
      "Millennial shape: (299, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vibing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text  label\n",
       "0    slay      0\n",
       "1     lit      0\n",
       "2     bet      0\n",
       "3    fire      0\n",
       "4  vibing      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cool</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dude</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buddy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text  label\n",
       "0  awesome      0\n",
       "1    chill      0\n",
       "2     cool      0\n",
       "3     dude      0\n",
       "4    buddy      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 2 â€” Load your data\n",
    "genz_df = pd.read_csv(\"Gen-Z - Sheet1.csv\")\n",
    "mill_df = pd.read_csv(\"old-ppl - Sheet1.csv\")\n",
    "\n",
    "print(\"Gen Z shape:\", genz_df.shape)\n",
    "print(\"Millennial shape:\", mill_df.shape)\n",
    "display(genz_df.head())\n",
    "display(mill_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff99f879-2f0d-4ae1-a6ba-8c9f8810a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 â€” Slang map (start small, extend as needed)\n",
    "# Add more entries from your dataset after inspecting rare tokens.\n",
    "slang_map = {\n",
    "    \"slay\": \"awesome\",\n",
    "    \"lit\": \"amazing\",\n",
    "    \"bet\": \"okay\",\n",
    "    \"fire\": \"great\",\n",
    "    \"vibing\": \"enjoying\",\n",
    "    \"cap\": \"lie\",\n",
    "    \"no cap\": \"no_lie\",\n",
    "    \"fr\": \"for_real\",\n",
    "    \"sus\": \"suspicious\",\n",
    "    \"delulu\": \"delusional\",\n",
    "    \"ratio\": \"disagree\",\n",
    "    \"tf\": \"what_the\",\n",
    "    \"idk\": \"i_do_not_know\",\n",
    "    \"ikr\": \"i_know_right\",\n",
    "    \"brb\": \"be_right_back\",\n",
    "    # add tokens as you see them\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efbaabb-b40a-4c29-b9a6-1e2fa3168ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 â€” Emoji map for common emoji -> text\n",
    "# Extend as you find new emojis in your data.\n",
    "emoji_map = {\n",
    "    \"ðŸ˜‚\": \"laughing\",\n",
    "    \"ðŸ˜­\": \"crying\",\n",
    "    \"ðŸ˜¡\": \"angry\",\n",
    "    \"ðŸ˜ \": \"angry\",\n",
    "    \"ðŸ˜Š\": \"happy\",\n",
    "    \"ðŸ™‚\": \"smile\",\n",
    "    \"ðŸ˜\": \"love\",\n",
    "    \"â¤ï¸\": \"love\",\n",
    "    \"ðŸ’€\": \"dead_funny\",\n",
    "    \"ðŸ¤¡\": \"clown\",\n",
    "    \"ðŸ™„\": \"annoyed\",\n",
    "    \"ðŸ˜’\": \"unimpressed\",\n",
    "    \"ðŸ”¥\": \"hot\",\n",
    "    \"âœ¨\": \"sparkle\",\n",
    "    \"ðŸ’…\": \"sass\",\n",
    "    \"ðŸ¤\": \"agreement\",\n",
    "    \"ðŸ‘\": \"thumbs_up\",\n",
    "    \"ðŸ‘Ž\": \"thumbs_down\",\n",
    "    \"ðŸ¤¬\": \"swearing\",\n",
    "    \"ðŸ˜¢\": \"sad\",\n",
    "    \"ðŸ˜…\": \"nervous_laugh\",\n",
    "    # add more as you encounter them\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f558c554-0b3f-4847-8945-357bc82d6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 â€” Preprocessing functions\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def replace_emojis(text):\n",
    "    # First try to use our emoji_map for quick replacements\n",
    "    for em, meaning in emoji_map.items():\n",
    "        if em in text:\n",
    "            text = text.replace(em, \" \" + meaning + \" \")\n",
    "    # Then convert any remaining emoji unicode to text fallback using emoji.demojize\n",
    "    # demojize outputs like :grinning_face: â€” we convert to words\n",
    "    try:\n",
    "        dem = emoji.demojize(text)\n",
    "        # replace :smiling_face: -> smiling_face\n",
    "        dem = re.sub(r':([a-zA-Z0-9_+-]+):', r' \\1 ', dem)\n",
    "        return dem\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def replace_slangs(text):\n",
    "    # word-boundary replace for slang map; lowercased\n",
    "    text = \" \" + text + \" \"\n",
    "    for s, rep in slang_map.items():\n",
    "        # handle multi-word slang like \"no cap\"\n",
    "        pattern = r'\\b' + re.escape(s.lower()) + r'\\b'\n",
    "        text = re.sub(pattern, \" \" + rep + \" \", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def clean_text(text, do_lemmatize=True, remove_stopwords=False):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    # replace emojis\n",
    "    text = replace_emojis(text)\n",
    "    # replace slang tokens\n",
    "    text = replace_slangs(text)\n",
    "    # remove urls, mentions, hashtags (but keep hashtag words)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    text = re.sub(r'#', ' ', text)\n",
    "    # remove punctuation except for underscore used in replacements\n",
    "    text = re.sub(r'[{}]'.format(re.escape(string.punctuation.replace('_', ''))), ' ', text)\n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = text.split()\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    if do_lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a278ad0-c9b2-4e23-9626-2cfb38405546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample GenZ cleaned:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slay</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lit</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bet</td>\n",
       "      <td>okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fire</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vibing</td>\n",
       "      <td>enjoying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chill</td>\n",
       "      <td>chill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aesthetic</td>\n",
       "      <td>aesthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wholesome</td>\n",
       "      <td>wholesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>valid</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>iconic</td>\n",
       "      <td>iconic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text clean_text\n",
       "0       slay    awesome\n",
       "1        lit    amazing\n",
       "2        bet       okay\n",
       "3       fire      great\n",
       "4     vibing   enjoying\n",
       "5      chill      chill\n",
       "6  aesthetic  aesthetic\n",
       "7  wholesome  wholesome\n",
       "8      valid      valid\n",
       "9     iconic     iconic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mill cleaned:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awesome</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chill</td>\n",
       "      <td>chill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cool</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dude</td>\n",
       "      <td>dude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buddy</td>\n",
       "      <td>buddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amazing</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lovely</td>\n",
       "      <td>lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>peace</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text clean_text\n",
       "0  awesome    awesome\n",
       "1    chill      chill\n",
       "2     cool       cool\n",
       "3     dude       dude\n",
       "4    buddy      buddy\n",
       "5  amazing    amazing\n",
       "6    great      great\n",
       "7   lovely     lovely\n",
       "8    happy      happy\n",
       "9    peace      peace"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top GenZ tokens: [('yeah', 39), ('you', 36), ('oh', 29), ('is', 23), ('youâ€™re', 20), ('the', 19), ('sure', 19), ('your', 17), ('wow', 17), ('a', 16), ('smirking_face', 15), ('just', 14), ('another', 14), ('love', 13), ('keep', 13), ('that', 12), ('i', 12), ('being', 12), ('grimacing_face', 12), ('this', 11), ('and', 11), ('relieved_face', 9), ('cry', 9), ('totally', 9), ('nervous_laugh', 9), ('energy', 8), ('to', 8), ('great', 7), ('chill', 7), ('vibe', 7)]\n",
      "Top Mill tokens: [('you', 55), ('youâ€™re', 31), ('a', 29), ('oh', 26), ('because', 26), ('laughing', 25), ('my', 24), ('your', 24), ('smirking_face', 23), ('love', 22), ('to', 22), ('annoyed', 22), ('wow', 20), ('another', 20), ('is', 19), ('grimacing_face', 19), ('the', 18), ('that', 17), ('i', 16), ('this', 16), ('like', 15), ('so', 14), ('how', 14), ('make', 13), ('for', 13), ('just', 13), ('â€”', 12), ('sure', 12), ('yes', 12), ('great', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 â€” Apply preprocessing and inspect\n",
    "genz_df['clean_text'] = genz_df['text'].apply(lambda x: clean_text(x, do_lemmatize=True, remove_stopwords=False))\n",
    "mill_df['clean_text'] = mill_df['text'].apply(lambda x: clean_text(x, do_lemmatize=True, remove_stopwords=False))\n",
    "\n",
    "# Quick checks\n",
    "print(\"Sample GenZ cleaned:\")\n",
    "display(genz_df[['text','clean_text']].head(10))\n",
    "print(\"Sample Mill cleaned:\")\n",
    "display(mill_df[['text','clean_text']].head(10))\n",
    "\n",
    "# Frequency of tokens â€” helpful to spot uncovered slang/emojis\n",
    "def top_tokens(series, n=30):\n",
    "    c = Counter()\n",
    "    for s in series.dropna():\n",
    "        c.update(s.split())\n",
    "    return c.most_common(n)\n",
    "\n",
    "print(\"Top GenZ tokens:\", top_tokens(genz_df['clean_text'], 30))\n",
    "print(\"Top Mill tokens:\", top_tokens(mill_df['clean_text'], 30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e80fd6-66b2-4f7a-ad9c-6b6f66180db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned files to /mnt/data/\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 â€” Export cleaned datasets for training stage\n",
    "genz_df.to_csv(\"genz_cleaned.csv\", index=False)\n",
    "mill_df.to_csv(\"mill_cleaned.csv\", index=False)\n",
    "print(\"Saved cleaned files to /mnt/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59499ccb-14be-4b01-9c6e-0862edc52e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 â€” Imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d1347c-93ba-45f7-b88e-471d7a11bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen-Z Data: (299, 3)\n",
      "Millennial Data: (299, 3)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 â€” Load cleaned datasets\n",
    "genz_df = pd.read_csv(\"genz_cleaned.csv\")\n",
    "mill_df = pd.read_csv(\"mill_cleaned.csv\")\n",
    "\n",
    "print(\"Gen-Z Data:\", genz_df.shape)\n",
    "print(\"Millennial Data:\", mill_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ba91f95-9e78-4365-9ce3-455edd7b4325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for GenZ and Millennials (please wait)...\n",
      "âœ… Embeddings ready! (No widget errors, fully clean run)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 10 â€” Generate sentence embeddings (MPNet + sentiment)\n",
    "# Clean + silent version (no widget warnings)\n",
    "# ==============================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # hide all warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# disable tqdm progress bars globally\n",
    "tqdm.disable = True\n",
    "\n",
    "# better embedding model for sarcasm + tone awareness\n",
    "model_embed = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "# sentiment model adds emotional cues (POSITIVE/NEGATIVE + confidence)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def get_embeddings_with_sentiment(texts):\n",
    "    # contextual sentence embeddings\n",
    "    emb = model_embed.encode(texts, show_progress_bar=False)\n",
    "\n",
    "    # sentiment direction (+1/-1) and strength\n",
    "    sentiments = sentiment_analyzer(texts)\n",
    "    sent_scores = np.array([[1 if s['label'] == 'POSITIVE' else -1, s['score']] for s in sentiments])\n",
    "\n",
    "    # merge embeddings + sentiment for richer representation\n",
    "    return np.concatenate((emb, sent_scores), axis=1)\n",
    "\n",
    "print(\"Generating embeddings for GenZ and Millennials (please wait)...\")\n",
    "\n",
    "X_genz = get_embeddings_with_sentiment(genz_df['clean_text'].tolist())\n",
    "y_genz = genz_df['label'].values\n",
    "\n",
    "X_mill = get_embeddings_with_sentiment(mill_df['clean_text'].tolist())\n",
    "y_mill = mill_df['label'].values\n",
    "\n",
    "print(\"âœ… Embeddings ready! (No widget errors, fully clean run)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55cdf439-ecfb-4f6a-a26f-57f5d431ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting GenZ & Millennial data...\n",
      "âœ… Data split complete.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 11 â€” Split into Train/Test\n",
    "# ==============================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"Splitting GenZ & Millennial data...\")\n",
    "\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_genz, y_genz, test_size=0.2, random_state=42)\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(X_mill, y_mill, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"âœ… Data split complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18496f87-182a-49a3-9382-d061d014fb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing base models...\n",
      "âœ… Models initialized.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 12 â€” Define Base Models\n",
    "# ==============================================\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"Initializing base models...\")\n",
    "\n",
    "svm_clf = SVC(probability=True, kernel='linear', C=1, random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=150, max_depth=10, random_state=42)\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=150, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"âœ… Models initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa7d68f8-38c0-453b-bca1-5fe69346b25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble for Gen-Z...\n",
      "âœ… Gen-Z Ensemble Accuracy: 0.900\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 13 â€” Train Base + Meta (Gen-Z)\n",
    "# ==============================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Training ensemble for Gen-Z...\")\n",
    "\n",
    "# train base models\n",
    "svm_clf.fit(Xg_train, yg_train)\n",
    "rf_clf.fit(Xg_train, yg_train)\n",
    "xgb_clf.fit(Xg_train, yg_train)\n",
    "\n",
    "# validation split\n",
    "Xg_train_sub, Xg_val, yg_train_sub, yg_val = train_test_split(Xg_train, yg_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# meta features\n",
    "p1 = svm_clf.predict_proba(Xg_val)[:, 1]\n",
    "p2 = rf_clf.predict_proba(Xg_val)[:, 1]\n",
    "p3 = xgb_clf.predict_proba(Xg_val)[:, 1]\n",
    "X_meta_g = np.column_stack((p1, p2, p3))\n",
    "\n",
    "meta_clf_g = LogisticRegression(max_iter=1000)\n",
    "meta_clf_g.fit(X_meta_g, yg_val)\n",
    "\n",
    "# evaluate\n",
    "p1_test = svm_clf.predict_proba(Xg_test)[:, 1]\n",
    "p2_test = rf_clf.predict_proba(Xg_test)[:, 1]\n",
    "p3_test = xgb_clf.predict_proba(Xg_test)[:, 1]\n",
    "X_meta_test_g = np.column_stack((p1_test, p2_test, p3_test))\n",
    "pred_meta_g = meta_clf_g.predict(X_meta_test_g)\n",
    "\n",
    "print(f\"âœ… Gen-Z Ensemble Accuracy: {accuracy_score(yg_test, pred_meta_g):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f91bd69f-a684-4c90-be1d-404999c25376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble for Millennials...\n",
      "âœ… Millennial Ensemble Accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 14 â€” Train Base + Meta (Millennials)\n",
    "# ==============================================\n",
    "print(\"Training ensemble for Millennials...\")\n",
    "\n",
    "# base models\n",
    "svm_clf.fit(Xm_train, ym_train)\n",
    "rf_clf.fit(Xm_train, ym_train)\n",
    "xgb_clf.fit(Xm_train, ym_train)\n",
    "\n",
    "# validation\n",
    "Xm_train_sub, Xm_val, ym_train_sub, ym_val = train_test_split(Xm_train, ym_train, test_size=0.2, random_state=42)\n",
    "\n",
    "p1 = svm_clf.predict_proba(Xm_val)[:, 1]\n",
    "p2 = rf_clf.predict_proba(Xm_val)[:, 1]\n",
    "p3 = xgb_clf.predict_proba(Xm_val)[:, 1]\n",
    "X_meta_m = np.column_stack((p1, p2, p3))\n",
    "\n",
    "meta_clf_m = LogisticRegression(max_iter=1000)\n",
    "meta_clf_m.fit(X_meta_m, ym_val)\n",
    "\n",
    "# evaluate\n",
    "p1_test = svm_clf.predict_proba(Xm_test)[:, 1]\n",
    "p2_test = rf_clf.predict_proba(Xm_test)[:, 1]\n",
    "p3_test = xgb_clf.predict_proba(Xm_test)[:, 1]\n",
    "X_meta_test_m = np.column_stack((p1_test, p2_test, p3_test))\n",
    "pred_meta_m = meta_clf_m.predict(X_meta_test_m)\n",
    "\n",
    "print(f\"âœ… Millennial Ensemble Accuracy: {accuracy_score(ym_test, pred_meta_m):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4f557f6-7734-410b-ac06-e00522df7e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all trained models...\n",
      "ðŸ’¾ All models saved successfully â€” clean, quiet, and ready for deployment.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 15 â€” Save Trained Models\n",
    "# ==============================================\n",
    "import joblib\n",
    "\n",
    "print(\"Saving all trained models...\")\n",
    "\n",
    "# Gen-Z\n",
    "joblib.dump(svm_clf, \"svm_model_genz.pkl\")\n",
    "joblib.dump(rf_clf, \"rf_model_genz.pkl\")\n",
    "joblib.dump(xgb_clf, \"xgb_model_genz.pkl\")\n",
    "joblib.dump(meta_clf_g, \"meta_model_genz.pkl\")\n",
    "\n",
    "# Millennials (retrain for clean independent set)\n",
    "svm_mill = SVC(probability=True, kernel='linear', C=1, random_state=42)\n",
    "rf_mill = RandomForestClassifier(n_estimators=150, max_depth=10, random_state=42)\n",
    "xgb_mill = XGBClassifier(\n",
    "    n_estimators=150, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "svm_mill.fit(Xm_train, ym_train)\n",
    "rf_mill.fit(Xm_train, ym_train)\n",
    "xgb_mill.fit(Xm_train, ym_train)\n",
    "\n",
    "joblib.dump(svm_mill, \"svm_model_mill.pkl\")\n",
    "joblib.dump(rf_mill, \"rf_model_mill.pkl\")\n",
    "joblib.dump(xgb_mill, \"xgb_model_mill.pkl\")\n",
    "joblib.dump(meta_clf_m, \"meta_model_mill.pkl\")\n",
    "\n",
    "print(\"ðŸ’¾ All models saved successfully â€” clean, quiet, and ready for deployment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8326f06c-0034-4b53-b5e6-9eb3e74c0315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up emoji + slang normalization...\n",
      "Example â†’ bro laughing that roast was amazing dead funny\n",
      "âœ… Normalization layer ready.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 16 â€” Emoji + Slang Normalization Layer\n",
    "# ==============================================\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "print(\"Setting up emoji + slang normalization...\")\n",
    "\n",
    "emoji_map = {\n",
    "    \"ðŸ˜‚\": \"laughing\", \"ðŸ¤£\": \"laughing\", \"ðŸ˜­\": \"crying\", \"ðŸ˜¡\": \"angry\", \"ðŸ˜ \": \"angry\",\n",
    "    \"ðŸ’€\": \"dead funny\", \"â¤ï¸\": \"love\", \"ðŸ’”\": \"heartbroken\", \"ðŸ˜’\": \"annoyed\", \"ðŸ™„\": \"annoyed\",\n",
    "    \"ðŸ”¥\": \"fire\", \"ðŸ’…\": \"sassy\", \"ðŸ˜Ž\": \"cool\", \"ðŸ¤¡\": \"clown\", \"ðŸ¤¬\": \"offensive\", \"ðŸ˜ˆ\": \"evil\",\n",
    "    \"ðŸ¤”\": \"thinking\", \"ðŸ˜©\": \"tired\", \"ðŸ˜œ\": \"playful\", \"ðŸ˜‰\": \"flirty\"\n",
    "}\n",
    "\n",
    "slang_map = {\n",
    "    \"bruh\": \"bro\", \"idk\": \"i dont know\", \"lmao\": \"laughing\", \"rofl\": \"laughing\",\n",
    "    \"wtf\": \"what the hell\", \"smh\": \"disappointed\", \"omg\": \"oh my god\", \"af\": \"very\",\n",
    "    \"lit\": \"amazing\", \"slay\": \"killing it\", \"lowkey\": \"a little\", \"highkey\": \"very much\",\n",
    "    \"sus\": \"suspicious\", \"fr\": \"for real\", \"cap\": \"lie\", \"no cap\": \"truth\",\n",
    "    \"salty\": \"bitter\", \"tea\": \"gossip\", \"flex\": \"show off\"\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Cleans emojis, slang & punctuation â€” returns normalized lowercase text.\"\"\"\n",
    "    # Convert emojis to words\n",
    "    for emo, meaning in emoji_map.items():\n",
    "        text = text.replace(emo, f\" {meaning} \")\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Expand slang\n",
    "    for slang, expanded in slang_map.items():\n",
    "        pattern = r\"\\b\" + re.escape(slang) + r\"\\b\"\n",
    "        text = re.sub(pattern, expanded, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Clean punctuation & extra spaces\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "# quick sanity check\n",
    "sample = \"bruh ðŸ˜‚ that roast was lit ðŸ’€\"\n",
    "print(\"Example â†’\", normalize_text(sample))\n",
    "print(\"âœ… Normalization layer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ebaffa3-8161-425a-adf5-5d35039c46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models & embedding pipeline...\n",
      "âœ… All models & embedding layers loaded successfully.\n",
      "\n",
      "Gen-Z: {'label': 'Cyberbullying / Offensive', 'confidence': 90.2}\n",
      "Millennials: {'label': 'Non-offensive', 'confidence': 89.52}\n",
      "\n",
      "âœ… Unified prediction function working perfectly.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ðŸ§© Cell 17 â€” Unified Prediction Function\n",
    "# ==============================================\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Loading models & embedding pipeline...\")\n",
    "\n",
    "# load models\n",
    "svm_genz = joblib.load(\"svm_model_genz.pkl\")\n",
    "rf_genz = joblib.load(\"rf_model_genz.pkl\")\n",
    "xgb_genz = joblib.load(\"xgb_model_genz.pkl\")\n",
    "meta_genz = joblib.load(\"meta_model_genz.pkl\")\n",
    "\n",
    "svm_mill = joblib.load(\"svm_model_mill.pkl\")\n",
    "rf_mill = joblib.load(\"rf_model_mill.pkl\")\n",
    "xgb_mill = joblib.load(\"xgb_model_mill.pkl\")\n",
    "meta_mill = joblib.load(\"meta_model_mill.pkl\")\n",
    "\n",
    "model_embed = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "print(\"âœ… All models & embedding layers loaded successfully.\\n\")\n",
    "\n",
    "def predict_text(text: str, mode: str = \"genz\") -> dict:\n",
    "    \"\"\"\n",
    "    Predicts if text is offensive or non-offensive.\n",
    "    mode: 'genz' or 'mill' (for Millennials)\n",
    "    \"\"\"\n",
    "    # Step 1 â€” Normalize text\n",
    "    clean_text = normalize_text(text)\n",
    "\n",
    "    # Step 2 â€” Embed\n",
    "    # Step 2 â€” Embed (same as training)\n",
    "    base_emb = model_embed.encode([clean_text])\n",
    "    sentiment = sentiment_analyzer([clean_text])[0]\n",
    "    sent_dir = 1 if sentiment['label'] == 'POSITIVE' else -1\n",
    "    sent_conf = sentiment['score']\n",
    "    sent_feat = np.array([[sent_dir, sent_conf]])\n",
    "    emb = np.concatenate((base_emb, sent_feat), axis=1)\n",
    "\n",
    "\n",
    "    # Step 3 â€” Select correct ensemble\n",
    "    if mode.lower() == \"genz\":\n",
    "        p1 = svm_genz.predict_proba(emb)[:, 1]\n",
    "        p2 = rf_genz.predict_proba(emb)[:, 1]\n",
    "        p3 = xgb_genz.predict_proba(emb)[:, 1]\n",
    "        X_meta = np.column_stack((p1, p2, p3))\n",
    "        pred = meta_genz.predict(X_meta)[0]\n",
    "        conf = meta_genz.predict_proba(X_meta)[0][pred]\n",
    "    else:\n",
    "        p1 = svm_mill.predict_proba(emb)[:, 1]\n",
    "        p2 = rf_mill.predict_proba(emb)[:, 1]\n",
    "        p3 = xgb_mill.predict_proba(emb)[:, 1]\n",
    "        X_meta = np.column_stack((p1, p2, p3))\n",
    "        pred = meta_mill.predict(X_meta)[0]\n",
    "        conf = meta_mill.predict_proba(X_meta)[0][pred]\n",
    "\n",
    "    label = \"Cyberbullying / Offensive\" if pred == 1 else \"Non-offensive\"\n",
    "    return {\"label\": label, \"confidence\": round(float(conf) * 100, 2)}\n",
    "\n",
    "# sanity test\n",
    "sample1 = \"bruh ðŸ˜‚ u so dumb ðŸ’€\"\n",
    "sample2 = \"i love your outfit ðŸ’…\"\n",
    "print(\"Gen-Z:\", predict_text(sample1, mode=\"genz\"))\n",
    "print(\"Millennials:\", predict_text(sample2, mode=\"mill\"))\n",
    "print(\"\\nâœ… Unified prediction function working perfectly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba848201-9d72-4920-b973-cddb4e6dce44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
